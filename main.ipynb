{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Manteswami Kavya\n",
      "Saved article: Berthelsdorf Formation\n",
      "Saved article: Paraglaciecola arctica\n",
      "Saved article: Glory Glory (football chant)\n",
      "Saved article: Le Gheer\n",
      "Saved article: Edmond N'Tiamoah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:02<04:43,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Horatio Stockton Howell\n",
      "Saved article: 2019 World Junior Wrestling Championships\n",
      "Saved article: Consumer Electronics Control\n",
      "Saved article: Coal Gap School\n",
      "Saved article: Una Croce senza nome\n",
      "Saved article: Live and Electric at the Union Chapel\n",
      "Saved article: White-rumped tanager\n",
      "Saved article: Gila Wilderness\n",
      "Saved article: Hassan Farid Didi\n",
      "Saved article: Villa Wartholz\n",
      "Saved article: Matthew Murphy\n",
      "Saved article: Michigan goal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobori/anaconda3/envs/wiki/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/kobori/anaconda3/envs/wiki/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped disambiguation page: ['Democratic Renewal Party (Angola)', 'Democratic Renovator Party (Portugal)', 'Renovation (disambiguation)', 'Democratic Party (disambiguation)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:03<00:29,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Alileh Sar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:04<00:13,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Burnham Norton\n",
      "Saved article: Arnaud-François Lefèbvre\n",
      "Saved article: FIL European Luge Championships 2018\n",
      "Saved article: Boom (navigational barrier)\n",
      "Saved article: Napan, New Brunswick\n",
      "Saved article: Sovetskaya Street\n",
      "Saved article: Verougstraete\n",
      "Saved article: Roman Catholic Diocese of Laval\n",
      "Saved article: Territorial Abbey of Montevergine\n",
      "Saved article: Ricardo Cabrera Martínez\n",
      "Saved article: Bobr (urban-type settlement)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:05<00:11,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Blythe River (Tasmania)\n",
      "Skipped disambiguation page: ['Walter I of Brienne', 'Walter II of Brienne', 'Walter III of Brienne', 'Walter IV of Brienne', 'Walter V of Brienne', 'Walter VI of Brienne', 'Walter IV of Enghien', 'County of Brienne']\n",
      "Saved article: SMK Kok Lanas\n",
      "Saved article: Luc Argand\n",
      "Skipped disambiguation page: ['Buchanan County, Iowa', 'Buchanan County, Missouri', 'Buchanan County, Virginia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:05<00:10,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Pingyangmiao, You County\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:05<00:04, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: The Manchester Man (novel)\n",
      "Saved article: Baker Bridge train wreck\n",
      "Saved article: Mangelia barbadoides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:07<00:06,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Guy Wyndham\n",
      "Saved article: McCamley\n",
      "Saved article: You're Beautiful (Nathaniel Willemse song)\n",
      "Saved article: Olga James\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:07<00:05,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Parkenfestivalen\n",
      "Saved article: Deterministic memory\n",
      "Saved article: Carson City and Indian Village\n",
      "Saved article: Milia-like calcinosis\n",
      "Saved article: Loxocrambus mohaviellus\n",
      "Saved article: Chenar Bagali\n",
      "Saved article: Arnaud Desjardins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:07<00:03, 13.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: David Yencken\n",
      "Saved article: Gianclaudio Bressa\n",
      "Saved article: Jim Pena\n",
      "Saved article: Book Art\n",
      "Saved article: Airbus UK Broughton F.C.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:08<00:03, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped disambiguation page: ['Boot Hill (video game)', 'Boot Hill (film)', 'Boot Hill (role-playing game)', 'Boot Hill Bowl', 'Johnny Winter', 'Boot Hill', 'Boot Hill Museum', 'Glossary of cricket terms#B']\n",
      "Saved article: Casalvecchio Siculo\n",
      "Saved article: Rod Anderson (writer)\n",
      "Saved article: Hendren Building\n",
      "Saved article: Germany–Tanzania relations\n",
      "Saved article: Tindal Bluff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:09<00:07,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Fengxin Road station\n",
      "Saved article: European honey buzzard\n",
      "Saved article: Dorothy Brandon\n",
      "Saved article: 1049 Gotho\n",
      "Saved article: London 1980 International Stamp Exhibition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:09<00:06,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Rock Creek Park Golf Course\n",
      "Saved article: Safsaf massacre\n",
      "Saved article: Grotella septempunctata\n",
      "Saved article: Fear of the Digital Remix\n",
      "Saved article: Nothing but Hope and Passion\n",
      "Saved article: Alfred Worden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [00:09<00:01, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Oral pontine reticular nucleus\n",
      "Saved article: Barar Deh, Dodangeh\n",
      "Saved article: Uki waza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:10<00:01, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: 2016 KNSB Dutch Single Distance Championships – Women's 3000 m\n",
      "Saved article: 1963 Nova Scotia general election\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:11<00:02,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: USS Sagittarius\n",
      "Saved article: Matam Region\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:11<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Motor unit\n",
      "Saved article: Saint Michel d'Aiguilhe\n",
      "Saved article: 1995 CFL season\n",
      "Saved article: Rok Urbanc\n",
      "Saved article: Beach Park Isles\n",
      "Saved article: Mafalda of Castile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:11<00:00, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Herbert McCabe\n",
      "Saved article: Ahrue Luster\n",
      "Saved article: Peter Sainthill (died 1571)\n",
      "Skipped disambiguation page: ['David Zilberman (wrestler)', 'David B. Zilberman', 'David Zilberman (economist)']\n",
      "Saved article: Assistant Secretary of Defense for Health Affairs\n",
      "Saved article: William Rant\n",
      "Saved article: Somerset v Stewart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Joel Hitt\n",
      "Saved article: San Francisco Writers Grotto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import wikipedia\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# output_wikiディレクトリを作成\n",
    "os.makedirs(\"output_wiki\", exist_ok=True)\n",
    "\n",
    "# 全ての英語Wikipediaページのタイトルを取得する\n",
    "all_titles = list(wikipedia.random(pages=5000))  # 5000ページ分のタイトルを取得\n",
    "\n",
    "# 100個のタイトルをランダムに選択する\n",
    "selected_titles = random.sample(all_titles, 100)\n",
    "\n",
    "def download_page(args):\n",
    "    i, title = args\n",
    "    try:\n",
    "        page = wikipedia.page(title)\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        # 記事が見つからない場合はスキップ\n",
    "        return\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # 曖昧な記事はスキップする\n",
    "        print(f\"Skipped disambiguation page: {e.options}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    else:\n",
    "        filename = os.path.join(\"output_wiki\", f\"{i:03d}.txt\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(page.content)\n",
    "        print(f\"Saved article: {page.title}\")\n",
    "\n",
    "# 選択したタイトルの記事を取得し、保存する\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(download_page, enumerate(selected_titles, start=1)), total=len(selected_titles)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文の分割が完了しました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kobori/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# NLTKのデータをダウンロード\n",
    "nltk.download('punkt')\n",
    "\n",
    "# output_wikiディレクトリのパス\n",
    "wiki_dir = \"output_wiki\"\n",
    "\n",
    "# output_textディレクトリを作成\n",
    "output_dir = \"output_text\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# output_wikiディレクトリ内のファイルを処理\n",
    "for filename in os.listdir(wiki_dir):\n",
    "    # ファイルパスを構築\n",
    "    file_path = os.path.join(wiki_dir, filename)\n",
    "\n",
    "    # ファイルを読み込む\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # 記事を1文ずつに分割\n",
    "    sentences = sent_tokenize(content)\n",
    "\n",
    "    # output_textディレクトリ内にサブディレクトリを作成\n",
    "    sub_dir_name = os.path.splitext(filename)[0]\n",
    "    sub_dir_path = os.path.join(output_dir, sub_dir_name)\n",
    "    os.makedirs(sub_dir_path, exist_ok=True)\n",
    "\n",
    "    # 分割した文をファイルに保存\n",
    "    for i, sentence in enumerate(sentences, start=1):\n",
    "        sentence_filename = os.path.join(sub_dir_path, f\"{i:03d}.txt\")\n",
    "        with open(sentence_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(sentence)\n",
    "\n",
    "print(\"文の分割が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirectory 1/90: 100%|██████████| 3/3 [00:16<00:00,  5.33s/file]\n",
      "Processing subdirectory 2/90: 100%|██████████| 5/5 [00:17<00:00,  3.48s/file]\n",
      "Processing subdirectory 3/90: 100%|██████████| 51/51 [03:41<00:00,  4.34s/file]\n",
      "Processing subdirectory 4/90: 100%|██████████| 20/20 [01:43<00:00,  5.18s/file]\n",
      "Processing subdirectory 5/90: 100%|██████████| 13/13 [00:56<00:00,  4.35s/file]\n",
      "Processing subdirectory 6/90: 100%|██████████| 37/37 [04:22<00:00,  7.09s/file]\n",
      "Processing subdirectory 7/90: 100%|██████████| 3/3 [00:12<00:00,  4.15s/file]\n",
      "Processing subdirectory 8/90: 100%|██████████| 4/4 [00:16<00:00,  4.12s/file]\n",
      "Processing subdirectory 9/90: 100%|██████████| 7/7 [00:41<00:00,  5.92s/file]\n",
      "Processing subdirectory 10/90: 100%|██████████| 43/43 [04:24<00:00,  6.15s/file]\n",
      "Processing subdirectory 11/90: 100%|██████████| 21/21 [02:01<00:00,  5.77s/file]\n",
      "Processing subdirectory 12/90: 100%|██████████| 42/42 [05:06<00:00,  7.30s/file]\n",
      "Processing subdirectory 13/90: 100%|██████████| 31/31 [03:39<00:00,  7.07s/file]\n",
      "Processing subdirectory 14/90: 100%|██████████| 5/5 [00:28<00:00,  5.61s/file]\n",
      "Processing subdirectory 15/90: 100%|██████████| 3/3 [00:20<00:00,  6.95s/file]\n",
      "Processing subdirectory 16/90: 100%|██████████| 2/2 [00:18<00:00,  9.24s/file]\n",
      "Processing subdirectory 17/90: 100%|██████████| 8/8 [00:48<00:00,  6.02s/file]\n",
      "Processing subdirectory 18/90: 100%|██████████| 3/3 [00:11<00:00,  3.93s/file]\n",
      "Processing subdirectory 19/90: 100%|██████████| 14/14 [01:06<00:00,  4.73s/file]\n",
      "Processing subdirectory 20/90: 100%|██████████| 4/4 [00:12<00:00,  3.09s/file]\n",
      "Processing subdirectory 21/90: 100%|██████████| 10/10 [00:57<00:00,  5.75s/file]\n",
      "Processing subdirectory 22/90: 100%|██████████| 57/57 [04:53<00:00,  5.15s/file]\n",
      "Processing subdirectory 23/90: 100%|██████████| 2/2 [00:08<00:00,  4.03s/file]\n",
      "Processing subdirectory 24/90: 100%|██████████| 4/4 [00:44<00:00, 11.13s/file]\n",
      "Processing subdirectory 25/90: 100%|██████████| 134/134 [06:23<00:00,  2.87s/file]\n",
      "Processing subdirectory 26/90: 100%|██████████| 3/3 [00:17<00:00,  5.94s/file]\n",
      "Processing subdirectory 27/90: 100%|██████████| 20/20 [01:32<00:00,  4.63s/file]\n",
      "Processing subdirectory 28/90: 100%|██████████| 5/5 [00:15<00:00,  3.11s/file]\n",
      "Processing subdirectory 29/90: 100%|██████████| 5/5 [00:14<00:00,  2.86s/file]\n",
      "Processing subdirectory 30/90: 100%|██████████| 13/13 [01:02<00:00,  4.79s/file]\n",
      "Processing subdirectory 31/90: 100%|██████████| 21/21 [01:45<00:00,  5.01s/file]\n",
      "Processing subdirectory 32/90: 100%|██████████| 2/2 [00:07<00:00,  3.91s/file]\n",
      "Processing subdirectory 33/90: 100%|██████████| 6/6 [00:20<00:00,  3.40s/file]\n",
      "Processing subdirectory 34/90: 100%|██████████| 36/36 [04:09<00:00,  6.94s/file]\n",
      "Processing subdirectory 35/90: 100%|██████████| 43/43 [06:24<00:00,  8.94s/file]\n",
      "Processing subdirectory 36/90: 100%|██████████| 9/9 [01:02<00:00,  6.94s/file]\n",
      "Processing subdirectory 37/90: 100%|██████████| 239/239 [22:43<00:00,  5.70s/file]\n",
      "Processing subdirectory 38/90: 100%|██████████| 3/3 [00:21<00:00,  7.18s/file]\n",
      "Processing subdirectory 39/90: 100%|██████████| 29/29 [03:19<00:00,  6.87s/file]\n",
      "Processing subdirectory 40/90: 100%|██████████| 4/4 [00:22<00:00,  5.53s/file]\n",
      "Processing subdirectory 41/90: 100%|██████████| 3/3 [00:13<00:00,  4.36s/file]\n",
      "Processing subdirectory 42/90: 100%|██████████| 8/8 [00:34<00:00,  4.32s/file]\n",
      "Processing subdirectory 43/90: 100%|██████████| 3/3 [00:06<00:00,  2.23s/file]\n",
      "Processing subdirectory 44/90: 100%|██████████| 20/20 [01:59<00:00,  5.97s/file]\n",
      "Processing subdirectory 45/90: 100%|██████████| 40/40 [03:45<00:00,  5.65s/file]\n",
      "Processing subdirectory 46/90: 100%|██████████| 3/3 [00:39<00:00, 13.32s/file]\n",
      "Processing subdirectory 47/90: 100%|██████████| 14/14 [01:17<00:00,  5.52s/file]\n",
      "Processing subdirectory 48/90: 100%|██████████| 83/83 [06:27<00:00,  4.66s/file]\n",
      "Processing subdirectory 49/90: 100%|██████████| 15/15 [01:49<00:00,  7.32s/file]\n",
      "Processing subdirectory 50/90: 100%|██████████| 7/7 [01:10<00:00, 10.01s/file]\n",
      "Processing subdirectory 51/90: 100%|██████████| 10/10 [00:40<00:00,  4.10s/file]\n",
      "Processing subdirectory 52/90: 100%|██████████| 3/3 [00:19<00:00,  6.37s/file]\n",
      "Processing subdirectory 53/90: 100%|██████████| 3/3 [00:11<00:00,  3.79s/file]\n",
      "Processing subdirectory 54/90: 100%|██████████| 17/17 [01:21<00:00,  4.77s/file]\n",
      "Processing subdirectory 55/90: 100%|██████████| 29/29 [04:04<00:00,  8.44s/file]\n",
      "Processing subdirectory 56/90: 100%|██████████| 4/4 [00:13<00:00,  3.29s/file]\n",
      "Processing subdirectory 57/90: 100%|██████████| 14/14 [01:19<00:00,  5.69s/file]\n",
      "Processing subdirectory 58/90: 100%|██████████| 53/53 [02:54<00:00,  3.30s/file]\n",
      "Processing subdirectory 59/90: 100%|██████████| 6/6 [00:34<00:00,  5.69s/file]\n",
      "Processing subdirectory 60/90: 100%|██████████| 18/18 [01:31<00:00,  5.09s/file]\n",
      "Processing subdirectory 61/90: 100%|██████████| 56/56 [05:13<00:00,  5.60s/file]\n",
      "Processing subdirectory 62/90: 100%|██████████| 6/6 [00:28<00:00,  4.71s/file]\n",
      "Processing subdirectory 63/90: 100%|██████████| 59/59 [05:01<00:00,  5.12s/file]\n",
      "Processing subdirectory 64/90: 100%|██████████| 2/2 [00:10<00:00,  5.10s/file]\n",
      "Processing subdirectory 65/90: 100%|██████████| 3/3 [00:33<00:00, 11.12s/file]\n",
      "Processing subdirectory 66/90: 100%|██████████| 4/4 [00:20<00:00,  5.05s/file]\n",
      "Processing subdirectory 67/90: 100%|██████████| 24/24 [01:31<00:00,  3.80s/file]\n",
      "Processing subdirectory 68/90: 100%|██████████| 29/29 [02:05<00:00,  4.31s/file]\n",
      "Processing subdirectory 69/90: 100%|██████████| 7/7 [00:32<00:00,  4.58s/file]\n",
      "Processing subdirectory 70/90: 100%|██████████| 16/16 [01:21<00:00,  5.10s/file]\n",
      "Processing subdirectory 71/90: 100%|██████████| 3/3 [00:18<00:00,  6.14s/file]\n",
      "Processing subdirectory 72/90: 100%|██████████| 12/12 [04:11<00:00, 20.98s/file]\n",
      "Processing subdirectory 73/90: 100%|██████████| 22/22 [02:10<00:00,  5.95s/file]\n",
      "Processing subdirectory 74/90: 100%|██████████| 27/27 [02:25<00:00,  5.40s/file]\n",
      "Processing subdirectory 75/90: 100%|██████████| 13/13 [00:49<00:00,  3.77s/file]\n",
      "Processing subdirectory 76/90: 100%|██████████| 12/12 [00:49<00:00,  4.13s/file]\n",
      "Processing subdirectory 77/90: 100%|██████████| 23/23 [01:26<00:00,  3.78s/file]\n",
      "Processing subdirectory 78/90: 100%|██████████| 6/6 [00:39<00:00,  6.64s/file]\n",
      "Processing subdirectory 79/90: 100%|██████████| 28/28 [02:43<00:00,  5.85s/file]\n",
      "Processing subdirectory 80/90: 100%|██████████| 15/15 [01:38<00:00,  6.58s/file]\n",
      "Processing subdirectory 81/90: 100%|██████████| 2/2 [00:12<00:00,  6.28s/file]\n",
      "Processing subdirectory 82/90: 100%|██████████| 2/2 [00:10<00:00,  5.15s/file]\n",
      "Processing subdirectory 83/90: 100%|██████████| 53/53 [04:25<00:00,  5.02s/file]\n",
      "Processing subdirectory 84/90: 100%|██████████| 5/5 [00:51<00:00, 10.35s/file]\n",
      "Processing subdirectory 85/90: 100%|██████████| 19/19 [02:05<00:00,  6.60s/file]\n",
      "Processing subdirectory 86/90: 100%|██████████| 178/178 [18:54<00:00,  6.37s/file]\n",
      "Processing subdirectory 87/90: 100%|██████████| 24/24 [02:19<00:00,  5.83s/file]\n",
      "Processing subdirectory 88/90: 100%|██████████| 17/17 [01:10<00:00,  4.12s/file]\n",
      "Processing subdirectory 89/90: 100%|██████████| 19/19 [01:26<00:00,  4.54s/file]\n",
      "Processing subdirectory 90/90: 100%|██████████| 40/40 [04:55<00:00,  7.40s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻訳が完了しました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from tqdm import tqdm  # プログレスバーを表示するためのライブラリ\n",
    "\n",
    "# 翻訳モデルの設定\n",
    "model_name = 'facebook/mbart-large-50-many-to-many-mmt'\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# output_textディレクトリのパス\n",
    "input_dir = \"output_text\"\n",
    "\n",
    "# output_japanese_textディレクトリを作成\n",
    "output_dir = \"output_japanese_text\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# サブディレクトリの総数を取得\n",
    "total_subdirs = len(os.listdir(input_dir))\n",
    "\n",
    "# output_textディレクトリ内のサブディレクトリを処理\n",
    "for i, sub_dir_name in enumerate(os.listdir(input_dir), start=1):\n",
    "    sub_dir_path = os.path.join(input_dir, sub_dir_name)\n",
    "\n",
    "    # output_japanese_textディレクトリ内にサブディレクトリを作成\n",
    "    output_sub_dir_path = os.path.join(output_dir, sub_dir_name)\n",
    "    os.makedirs(output_sub_dir_path, exist_ok=True)\n",
    "\n",
    "    # サブディレクトリ内のファイルを処理\n",
    "    files = os.listdir(sub_dir_path)\n",
    "    for filename in tqdm(files, desc=f\"Processing subdirectory {i}/{total_subdirs}\", unit=\"file\"):\n",
    "        file_path = os.path.join(sub_dir_path, filename)\n",
    "\n",
    "        # ファイルを読み込む\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # 文章を小さな部分に分割\n",
    "        sentences = textwrap.wrap(content, width=100)\n",
    "\n",
    "        translated_sentences = []\n",
    "        for sent in sentences:\n",
    "            # 文章をトークナイザーでトークナイズし、モデルが理解できる形式に変換\n",
    "            inputs = tokenizer(sent, return_tensors=\"pt\")\n",
    "\n",
    "            # 翻訳の実行\n",
    "            generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"ja_XX\"])\n",
    "            translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            translated_sentences.append(translated[0])\n",
    "\n",
    "        # 翻訳された内容をファイルに保存\n",
    "        output_file_path = os.path.join(output_sub_dir_path, filename)\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for translation in translated_sentences:\n",
    "                f.write(translation + \"\\n\")\n",
    "\n",
    "print(\"翻訳が完了しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/souta-pqr/anaconda3/envs/B3_study/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "WARNING:root:It seems weight norm is not applied in the pretrained model but the current model uses it. To keep the compatibility, we remove the norm from the current model. This may cause unexpected behavior due to the parameter mismatch in finetuning. To avoid this issue, please change the following parameters in config to false:\n",
      " - discriminator_params.follow_official_norm\n",
      " - discriminator_params.scale_discriminator_params.use_weight_norm\n",
      " - discriminator_params.scale_discriminator_params.use_spectral_norm\n",
      "\n",
      "See also:\n",
      " - https://github.com/espnet/espnet/pull/5240\n",
      " - https://github.com/espnet/espnet/pull/5249\n",
      "Processing 078:  67%|██████▋   | 2/3 [00:21<00:09,  9.85s/file]WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n",
      "Processing 078: 100%|██████████| 3/3 [00:22<00:00,  7.62s/file]\n",
      "Processing 080:  80%|████████  | 4/5 [00:27<00:05,  5.12s/file]WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n",
      "Processing 080: 100%|██████████| 5/5 [00:30<00:00,  6.05s/file]\n",
      "Processing 074:   4%|▍         | 2/51 [00:14<05:28,  6.70s/file]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "import soundfile as sf\n",
    "\n",
    "# モデル名\n",
    "model_tag = \"kan-bayashi/jsut_full_band_vits_prosody\"\n",
    "vocoder_tag = \"parallel_wavegan/jsut_parallel_wavegan.v1\"\n",
    "\n",
    "# 音声合成器の生成\n",
    "text2speech = Text2Speech.from_pretrained(\n",
    "    model_tag=model_tag,\n",
    "    vocoder_tag=vocoder_tag,\n",
    ")\n",
    "\n",
    "# output_japanese_textディレクトリのパス\n",
    "input_dir = \"output_japanese_text\"\n",
    "\n",
    "# output_audioディレクトリを作成\n",
    "output_dir = \"output_audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# output_japanese_textディレクトリ内のサブディレクトリを処理\n",
    "for sub_dir_name in os.listdir(input_dir):\n",
    "    sub_dir_path = os.path.join(input_dir, sub_dir_name)\n",
    "    \n",
    "    # output_audioディレクトリ内にサブディレクトリを作成\n",
    "    output_sub_dir_path = os.path.join(output_dir, sub_dir_name)\n",
    "    os.makedirs(output_sub_dir_path, exist_ok=True)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを処理\n",
    "    files = os.listdir(sub_dir_path)\n",
    "    for i, filename in enumerate(tqdm(sorted(files), desc=f\"Processing {sub_dir_name}\", unit=\"file\"), start=1):\n",
    "        file_path = os.path.join(sub_dir_path, filename)\n",
    "        \n",
    "        # ファイルを読み込む\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 音声合成\n",
    "        result = text2speech(content)\n",
    "        \n",
    "        # 音声ファイルの保存\n",
    "        output_file_path = os.path.join(output_sub_dir_path, f\"{i:03}.wav\")\n",
    "        sf.write(output_file_path, result[\"wav\"], text2speech.fs, \"PCM_16\")\n",
    "\n",
    "print(\"音声ファイルの作成が完了しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
