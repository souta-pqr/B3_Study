{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Aberangell\n",
      "Saved article: Arucas, Las Palmas\n",
      "Saved article: 643 Scheherezade\n",
      "Saved article: Hannah Jadagu\n",
      "Saved article: Andrea Ceccherini\n",
      "Saved article: Manfred Tiedtke\n",
      "Saved article: Sirthauli\n",
      "Saved article: Kolound\n",
      "Saved article: Anoplognathus pallidicollis\n",
      "Saved article: Alfred Sands Pell\n",
      "Saved article: Serge Lamothe\n",
      "Saved article: Foxtrot Uniform Charlie Kilo\n",
      "Saved article: Mo Myeong-hui\n",
      "Saved article: Jessica Morden\n",
      "Saved article: Irvington Historic District (Irvington, Kentucky)\n",
      "Saved article: Herbes de Provence\n",
      "Saved article: Āmir ibn Abī al-Bukayr\n",
      "Saved article: Somerset Area High School\n",
      "Saved article: Central Uplands\n",
      "Skipped disambiguation page: ['Sīmǎ Yǎn', 'Sīmǎ Yàn', 'Emperor Cheng of Jin']\n",
      "Saved article: Same-sex marriage in Zacatecas\n",
      "Saved article: Pyrrole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:02<03:29,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Association of Cricket Officials\n",
      "Saved article: Eastern Region of British Railways\n",
      "Skipped disambiguation page: ['Dionne (name)', 'Centre Marcel Dionne', 'Dionne Lake', 'Dionne quintuplets', 'USS Dionne (DE-261)', 'Crystal Ball (box set)', 'Dion (disambiguation)', 'Dione (disambiguation)']\n",
      "Saved article: National Support Element\n",
      "Saved article: League of Ireland First Division\n",
      "Skipped disambiguation page: ['Stari Grad, Sarajevo', 'Stari Grad, Croatia', 'Stari Grad, Šibenik', 'Stari grad, Cetinje', 'Stara Varoš, Podgorica', 'Stari Grad, Čaška', 'Stari Grad, Belgrade', 'Stari Grad, Novi Sad', 'Stari Grad, Užice', 'Stari Grad, Kragujevac', 'Ortnek Castle', 'Stari Grad, Makole', 'Starigrad (disambiguation)', 'Novi Grad (disambiguation)', 'Gornji Grad (disambiguation)', 'Donji Grad (disambiguation)', 'Grad (toponymy)', 'Staro Selo (disambiguation)']\n",
      "Saved article: Theatre Royal, Drury Lane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:02<00:15,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped disambiguation page: ['Latin alpha', 'International Phonetic Alphabet', 'General Alphabet of Cameroon Languages', 'NATO phonetic alphabet', 'Alpha (Lombard)', 'The Alphas', 'anthologies edited by Robert Silverberg', 'Alpha (2018 film)', 'Alpha (2019 film)', 'Alpha (2025 French film)', 'Alpha (2025 Hindi film)', 'Alpha Alpha', 'Alphas', '\"Alpha\" (The X-Files)', 'Alpha and Omega (radio plays)', 'Alpha TV', 'Alpha (DC Comics)', 'Alpha (Marvel Comics)', 'Alpha (The Walking Dead)', 'Alpha 4 (Power Rangers)', 'Alpha the Ultimate Mutant', 'Moonbase Alpha (Space: 1999)', 'Storks', 'Up', 'Alpha Trion', 'Alpha', \"Ryan's World\", 'Xenoblade Chronicles 3: Future Redeemed', 'Alpha (Magic: The Gathering)', 'Alpha (video game)', 'Airlock Alpha', 'Outhere', 'octatonic scale', 'Alpha (band)', 'Alpha (Kazakh band)', 'Alpha (Aitana album)', 'Alpha (Alice Nine album)', 'Alpha (Asia album)', 'Alpha (Charlotte Day Wilson album)', 'Alpha (CL album)', 'Alpha (Jelena Karleuša album)', 'Alpha (Selena y Los Dinos album)', 'Alpha (Sevendust album)', 'Alpha (Shenseea album)', 'Alpha (War of Ages album)', '\"Alpha\" (song)', 'Albedo 0.39', 'Minecraft – Volume Beta', 'Alpha Bank', 'Alpha Books', 'Alpha Industries', 'Alpha Media', 'Alpha TV', 'Alpha Video', 'Legendary Entertainment', 'Alpha (Australian magazine)', 'Absolute Return + Alpha', 'Alpha (given name)', 'Elizabeth Alpha-Lavalie', 'Jenny Alpha', 'Alpha Lee', 'Alpha, California', 'Alpha, Illinois', 'Alpha, Iowa', 'Alpha, Kentucky', 'Alpha, Maryland', 'Alpha, Michigan', 'Alpha, Minnesota', 'Alpha, Missouri', 'Alpha, New Jersey', 'Alpha, Ohio', 'Alpha, Oregon', 'Alpha (Morristown, Tennessee)', 'Alpha, Texas', 'Alpha, Virginia', 'Alpha, Washington', 'Alpha, Wisconsin', 'Alpha Ridge, Alaska', 'Alpha, Queensland', 'Alpha Ridge', 'Alpha (ethology)', 'Alpha helixes', 'Adrenergic receptor', 'SARS-CoV-2 Alpha variant', 'Immunoglobulin heavy chains', 'Trinchesia alpha', 'Ponera alpha', 'Alpha (programming language)', 'Alpha compositing', 'software release life cycle', 'AlphaServer', 'AlphaStation', 'DEC Alpha', 'Wolfram Alpha', 'Alpha (finance)', \"Cronbach's alpha\", 'Angular eccentricity', 'azimuth', 'Feigenbaum constants', 'type I error', 'significance level', 'inverse Ackermann function', 'ordinal numbers', 'ALPHA', 'ALPHA Collaboration', 'Alpha particle', 'Alpha waves', 'Angle of attack', 'Angular acceleration', 'Coefficient of thermal expansion', 'Fine-structure constant', 'Thermal diffusivity', 'transistor', 'geochemistry', 'Bayer designation', 'Alpha (navigation)', 'Minolta Alpha', 'Sony α', 'Samsung Galaxy Alpha', 'GM Alpha platform', 'Alpha (sternwheeler)', 'Advance Alpha', 'Alpha 2000', 'Dassault/Dornier Alpha Jet', 'Firefly Alpha', 'Kohler Alpha', 'Pipistrel Alpha Trainer', 'Subtropical Storm Alpha (1972)', 'Tropical Storm Alpha (2005)', 'Subtropical Storm Alpha (2020)', 'ALPHA (drug)', 'NATO phonetic alphabet', 'Generation Alpha', 'Alpha course', 'Alpha Group', 'Alpha Secondary School', 'Alpha Boys School', 'uniforms of the United States Marine Corps', 'All pages with titles beginning with Alpha', 'All pages with titles containing Alpha', 'Alpha and Omega (disambiguation)', 'Alpha value (disambiguation)', 'Alfa (disambiguation)', 'Moonbase Alpha (disambiguation)', 'A', 'Space Station Alpha', 'Space Complex Alpha', 'Beginning (disambiguation)', 'Birth (disambiguation)', 'Construction (disambiguation)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:02<00:10,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Ptilometra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:03<00:05, 11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: MTV Unplugged (Katy Perry EP)\n",
      "Saved article: Lê Văn Triết\n",
      "Saved article: Beneš-Mráz Bibi\n",
      "Saved article: Sebadoh discography\n",
      "Saved article: Qais Bin Abdul Munim Al Zawawi\n",
      "Saved article: Soutpansberg Military Area\n",
      "Saved article: Science of Logic\n",
      "Saved article: Eois expressaria\n",
      "Saved article: David Jackson (judge)\n",
      "Saved article: Galeazzo Gualdo Priorato\n",
      "Saved article: Rumuodomaya\n",
      "Saved article: Structural system\n",
      "Saved article: Mohammad Abdul Halim\n",
      "Saved article: Mikhail Sokolov\n",
      "Saved article: Pretorius\n",
      "Saved article: Nanninga\n",
      "Saved article: MSCI Hong Kong Index\n",
      "Saved article: Ilocos Sur's 3rd congressional district\n",
      "Saved article: 2011 Gabonese protests\n",
      "Skipped disambiguation page: ['Jurruru people', 'Jurruru language', 'Jururu language (Brazil)']\n",
      "Skipped disambiguation page: ['1934 Hemsworth by-election', '1946 Hemsworth by-election', '1991 Hemsworth by-election', '1996 Hemsworth by-election', 'Hemsworth (UK Parliament constituency)', 'UK Parliament']\n",
      "Saved article: Charlie Miller (North Carolina politician)\n",
      "Skipped disambiguation page: ['Alcamenes', 'Alcmenes', 'Alcamenes, son of Sthenelaides']\n",
      "Saved article: Maldives national football team\n",
      "Saved article: 2022–23 Burgos CF season\n",
      "Saved article: Swimming at the 2005 World Aquatics Championships – Men's 1500 metre freestyle\n",
      "Saved article: Yūko Kakazu\n",
      "Saved article: Just Heroes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:04<00:06,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Summit Meeting (Eric Alexander album)\n",
      "Saved article: Canouan Airport\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:05<00:01, 18.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: RAS syndrome\n",
      "Saved article: Kjell Engebretsen\n",
      "Saved article: David Grenvold\n",
      "Saved article: Newmarket Public Library (Ontario)\n",
      "Saved article: Shiwuliyuan\n",
      "Saved article: Perevalivka\n",
      "Saved article: Bechgaard\n",
      "Saved article: Shullat and Hanish\n",
      "Saved article: Baule\n",
      "Saved article: Winning Arrow\n",
      "Saved article: Cheshmeh Jalal\n",
      "Saved article: Ina Jaffe\n",
      "Saved article: Titusville, New York\n",
      "Saved article: Marie Denizard\n",
      "Saved article: Aksakovo, Vladimir Oblast\n",
      "Saved article: 1964 United States House of Representatives election in Delaware\n",
      "Saved article: Peter O'Neill\n",
      "Skipped disambiguation page: ['Kista', 'Kista borough', 'Kista metro station', 'Kista Galleria', 'Kista Science Tower', 'Kista Torn', 'Kista Nunatak', 'Kista Rock', 'Kista Strait']\n",
      "Saved article: Viswa Bharathi English Medium High School\n",
      "Saved article: The Gripping Hand\n",
      "Saved article: Troglochaetus\n",
      "Saved article: 3963 Paradzhanov\n",
      "Saved article: Cwench All Canadian Games\n",
      "Saved article: Gheorghe Mititelu\n",
      "Saved article: SumBingTik Festival\n",
      "Saved article: People's Action Party\n",
      "Saved article: Franz Roggow\n",
      "Saved article: Espeh Riz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:07<00:04,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Jean Houston\n",
      "Saved article: A Dangerous Method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:08<00:03,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Sphere of influence (astrodynamics)\n",
      "Saved article: Tom Harmon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Rivare, Indiana\n",
      "Saved article: Peter Eriksson (curler)\n",
      "Saved article: Teatro Cultura Artística\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import wikipedia\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# output_wikiディレクトリを作成\n",
    "os.makedirs(\"output_wiki\", exist_ok=True)\n",
    "\n",
    "# 全ての英語Wikipediaページのタイトルを取得する\n",
    "all_titles = list(wikipedia.random(pages=5000))  # 5000ページ分のタイトルを取得\n",
    "\n",
    "# 100個のタイトルをランダムに選択する\n",
    "selected_titles = random.sample(all_titles, 100)\n",
    "\n",
    "def download_page(args):\n",
    "    i, title = args\n",
    "    try:\n",
    "        page = wikipedia.page(title)\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        # 記事が見つからない場合はスキップ\n",
    "        return\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # 曖昧な記事はスキップする\n",
    "        print(f\"Skipped disambiguation page: {e.options}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    else:\n",
    "        filename = os.path.join(\"output_wiki\", f\"{i:03d}.txt\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(page.content)\n",
    "        print(f\"Saved article: {page.title}\")\n",
    "\n",
    "# 選択したタイトルの記事を取得し、保存する\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(download_page, enumerate(selected_titles, start=1)), total=len(selected_titles)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kobori/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/kobori/nltk_data'\n    - '/home/kobori/.conda/envs/b3study/nltk_data'\n    - '/home/kobori/.conda/envs/b3study/share/nltk_data'\n    - '/home/kobori/.conda/envs/b3study/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 記事を1文ずつに分割\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# output_textディレクトリ内にサブディレクトリを作成\u001b[39;00m\n\u001b[1;32m     28\u001b[0m sub_dir_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/b3study/lib/python3.9/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.conda/envs/b3study/lib/python3.9/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/b3study/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/b3study/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.conda/envs/b3study/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/kobori/nltk_data'\n    - '/home/kobori/.conda/envs/b3study/nltk_data'\n    - '/home/kobori/.conda/envs/b3study/share/nltk_data'\n    - '/home/kobori/.conda/envs/b3study/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# NLTKのデータを明示的なパスにダウンロードし、パスを追加する\n",
    "nltk_data_dir = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# punktをダウンロードして確認\n",
    "print(\"Downloading punkt tokenizer...\")\n",
    "download_result = nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "print(f\"Download result: {download_result}\")\n",
    "print(f\"NLTK data path: {nltk.data.path}\")\n",
    "\n",
    "# output_wikiディレクトリのパス\n",
    "wiki_dir = \"output_wiki\"\n",
    "\n",
    "# output_textディレクトリを作成\n",
    "output_dir = \"output_text\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# output_wikiディレクトリ内のファイルを処理\n",
    "for filename in os.listdir(wiki_dir):\n",
    "    # ファイルパスを構築\n",
    "    file_path = os.path.join(wiki_dir, filename)\n",
    "    \n",
    "    # ファイルかどうか確認\n",
    "    if not os.path.isfile(file_path):\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing file: {filename}\")\n",
    "\n",
    "    # ファイルを読み込む\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    try:\n",
    "        # 記事を1文ずつに分割\n",
    "        sentences = sent_tokenize(content)\n",
    "        \n",
    "        # output_textディレクトリ内にサブディレクトリを作成\n",
    "        sub_dir_name = os.path.splitext(filename)[0]\n",
    "        sub_dir_path = os.path.join(output_dir, sub_dir_name)\n",
    "        os.makedirs(sub_dir_path, exist_ok=True)\n",
    "\n",
    "        # 分割した文をファイルに保存\n",
    "        for i, sentence in enumerate(sentences, start=1):\n",
    "            sentence_filename = os.path.join(sub_dir_path, f\"{i:03d}.txt\")\n",
    "            with open(sentence_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(sentence)\n",
    "        \n",
    "        print(f\"Processed {filename}: {len(sentences)} sentences extracted\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {str(e)}\")\n",
    "\n",
    "print(\"文の分割が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirectory 1/100:  67%|██████▋   | 2/3 [00:08<00:04,  4.25s/file]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sent, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 翻訳の実行\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang_code_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mja_XX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m translated \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m translated_sentences\u001b[38;5;241m.\u001b[39mappend(translated[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/transformers/generation/utils.py:1655\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1649\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1650\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1651\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1652\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1653\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1655\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/transformers/generation/utils.py:3171\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   3168\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[1;32m   3170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3179\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:1576\u001b[0m, in \u001b[0;36mMBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1557\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m   1559\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1560\u001b[0m     input_ids,\n\u001b[1;32m   1561\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1575\u001b[0m )\n\u001b[0;32m-> 1576\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1578\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from tqdm import tqdm  # プログレスバーを表示するためのライブラリ\n",
    "\n",
    "# 翻訳モデルの設定\n",
    "model_name = 'facebook/mbart-large-50-many-to-many-mmt'\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# output_textディレクトリのパス\n",
    "input_dir = \"output_text\"\n",
    "\n",
    "# output_japanese_textディレクトリを作成\n",
    "output_dir = \"output_japanese_text\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# サブディレクトリの総数を取得\n",
    "total_subdirs = len(os.listdir(input_dir))\n",
    "\n",
    "# output_textディレクトリ内のサブディレクトリを処理\n",
    "for i, sub_dir_name in enumerate(os.listdir(input_dir), start=1):\n",
    "    sub_dir_path = os.path.join(input_dir, sub_dir_name)\n",
    "\n",
    "    # output_japanese_textディレクトリ内にサブディレクトリを作成\n",
    "    output_sub_dir_path = os.path.join(output_dir, sub_dir_name)\n",
    "    os.makedirs(output_sub_dir_path, exist_ok=True)\n",
    "\n",
    "    # サブディレクトリ内のファイルを処理\n",
    "    files = os.listdir(sub_dir_path)\n",
    "    for filename in tqdm(files, desc=f\"Processing subdirectory {i}/{total_subdirs}\", unit=\"file\"):\n",
    "        file_path = os.path.join(sub_dir_path, filename)\n",
    "\n",
    "        # ファイルを読み込む\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # 文章を小さな部分に分割\n",
    "        sentences = textwrap.wrap(content, width=100)\n",
    "\n",
    "        translated_sentences = []\n",
    "        for sent in sentences:\n",
    "            # 文章をトークナイザーでトークナイズし、モデルが理解できる形式に変換\n",
    "            inputs = tokenizer(sent, return_tensors=\"pt\")\n",
    "\n",
    "            # 翻訳の実行\n",
    "            generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"ja_XX\"])\n",
    "            translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            translated_sentences.append(translated[0])\n",
    "\n",
    "        # 翻訳された内容をファイルに保存\n",
    "        output_file_path = os.path.join(output_sub_dir_path, filename)\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for translation in translated_sentences:\n",
    "                f.write(translation + \"\\n\")\n",
    "\n",
    "print(\"翻訳が完了しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/souta-pqr/anaconda3/envs/w2t/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "https://zenodo.org/record/5521340/files/tts_train_full_band_vits_raw_phn_jaconv_pyopenjtalk_prosody_train.total_count.ave.zip?download=1: 100%|██████████| 357M/357M [19:23<00:00, 322kB/s]    \n",
      "/home/souta-pqr/anaconda3/envs/w2t/lib/python3.9/site-packages/espnet_model_zoo/downloader.py:369: UserWarning: Not validating checksum\n",
      "  warnings.warn(\"Not validating checksum\")\n",
      "/home/souta-pqr/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/souta-pqr/anaconda3/envs/w2t/lib/python3.9/site-packages/espnet2/gan_tts/vits/monotonic_align/__init__.py:19: UserWarning: Cython version is not available. Fallback to 'EXPERIMETAL' numba version. If you want to use the cython version, please build it as follows: `cd espnet2/gan_tts/vits/monotonic_align; python setup.py build_ext --inplace`\n",
      "  warnings.warn(\n",
      "WARNING:root:It seems weight norm is not applied in the pretrained model but the current model uses it. To keep the compatibility, we remove the norm from the current model. This may cause unexpected behavior due to the parameter mismatch in finetuning. To avoid this issue, please change the following parameters in config to false:\n",
      " - discriminator_params.follow_official_norm\n",
      " - discriminator_params.scale_discriminator_params.use_weight_norm\n",
      " - discriminator_params.scale_discriminator_params.use_spectral_norm\n",
      "\n",
      "See also:\n",
      " - https://github.com/espnet/espnet/pull/5240\n",
      " - https://github.com/espnet/espnet/pull/5249\n",
      "Processing 078:   0%|          | 0/3 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/r9y9/open_jtalk/releases/download/v1.11.1/open_jtalk_dic_utf_8-1.11.tar.gz\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dic.tar.gz: 100%|██████████| 22.6M/22.6M [00:03<00:00, 7.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar file /home/souta-pqr/anaconda3/envs/w2t/lib/python3.9/site-packages/pyopenjtalk/dic.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 078:  67%|██████▋   | 2/3 [00:31<00:13, 14.00s/file]WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n",
      "Processing 078: 100%|██████████| 3/3 [00:32<00:00, 10.93s/file]\n",
      "Processing 080:  20%|██        | 1/5 [00:25<01:40, 25.11s/file]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 音声合成\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtext2speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 音声ファイルの保存\u001b[39;00m\n\u001b[1;32m     44\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_sub_dir_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/espnet2/bin/tts_inference.py:196\u001b[0m, in \u001b[0;36mText2Speech.__call__\u001b[0;34m(self, text, speech, durations, spembs, sids, lids, decode_conf)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malways_fix_seed:\n\u001b[1;32m    195\u001b[0m     set_all_random_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 196\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minference(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# calculate additional metrics\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matt_w\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/espnet2/tts/espnet_model.py:297\u001b[0m, in \u001b[0;36mESPnetTTSModel.inference\u001b[0;34m(self, text, speech, spembs, sids, lids, durations, pitch, energy, **decode_config)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     input_dict\u001b[38;5;241m.\u001b[39mupdate(lids\u001b[38;5;241m=\u001b[39mlids)\n\u001b[0;32m--> 297\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecode_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m output_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# NOTE: normalize.inverse is in-place operation\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     feat_gen_denorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize\u001b[38;5;241m.\u001b[39minverse(\n\u001b[1;32m    302\u001b[0m         output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    303\u001b[0m     )[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/espnet2/gan_tts/vits/vits.py:636\u001b[0m, in \u001b[0;36mVITS.inference\u001b[0;34m(self, text, feats, sids, spembs, lids, durations, noise_scale, noise_scale_dur, alpha, max_len, use_teacher_forcing)\u001b[0m\n\u001b[1;32m    624\u001b[0m     wav, att_w, dur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39minference(\n\u001b[1;32m    625\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m    626\u001b[0m         text_lengths\u001b[38;5;241m=\u001b[39mtext_lengths,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m         use_teacher_forcing\u001b[38;5;241m=\u001b[39muse_teacher_forcing,\n\u001b[1;32m    634\u001b[0m     )\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     wav, att_w, dur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspembs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspembs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdur\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdurations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_scale_dur\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_scale_dur\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(wav\u001b[38;5;241m=\u001b[39mwav\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), att_w\u001b[38;5;241m=\u001b[39matt_w[\u001b[38;5;241m0\u001b[39m], duration\u001b[38;5;241m=\u001b[39mdur[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/espnet2/gan_tts/vits/generator.py:548\u001b[0m, in \u001b[0;36mVITSGenerator.inference\u001b[0;34m(self, text, text_lengths, feats, feats_lengths, sids, spembs, lids, dur, noise_scale, noise_scale_dur, alpha, max_len, use_teacher_forcing)\u001b[0m\n\u001b[1;32m    546\u001b[0m     z_p \u001b[38;5;241m=\u001b[39m m_p \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(m_p) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(logs_p) \u001b[38;5;241m*\u001b[39m noise_scale\n\u001b[1;32m    547\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow(z_p, y_mask, g\u001b[38;5;241m=\u001b[39mg, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 548\u001b[0m     wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wav\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), attn\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), dur\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/w2t/lib/python3.9/site-packages/espnet2/gan_tts/hifigan/hifigan.py:156\u001b[0m, in \u001b[0;36mHiFiGANGenerator.forward\u001b[0;34m(self, c, g)\u001b[0m\n\u001b[1;32m    154\u001b[0m     cs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \u001b[38;5;66;03m# initialize\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_blocks):\n\u001b[0;32m--> 156\u001b[0m         cs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_blocks \u001b[38;5;241m+\u001b[39m j](c)\n\u001b[1;32m    157\u001b[0m     c \u001b[38;5;241m=\u001b[39m cs \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_blocks\n\u001b[1;32m    158\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv(c)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "import soundfile as sf\n",
    "\n",
    "# モデル名\n",
    "model_tag = \"kan-bayashi/jsut_full_band_vits_prosody\"\n",
    "vocoder_tag = \"parallel_wavegan/jsut_parallel_wavegan.v1\"\n",
    "\n",
    "# 音声合成器の生成\n",
    "text2speech = Text2Speech.from_pretrained(\n",
    "    model_tag=model_tag,\n",
    "    vocoder_tag=vocoder_tag,\n",
    ")\n",
    "\n",
    "# output_japanese_textディレクトリのパス\n",
    "input_dir = \"output_japanese_text\"\n",
    "\n",
    "# output_audioディレクトリを作成\n",
    "output_dir = \"output_audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# output_japanese_textディレクトリ内のサブディレクトリを処理\n",
    "for sub_dir_name in os.listdir(input_dir):\n",
    "    sub_dir_path = os.path.join(input_dir, sub_dir_name)\n",
    "    \n",
    "    # output_audioディレクトリ内にサブディレクトリを作成\n",
    "    output_sub_dir_path = os.path.join(output_dir, sub_dir_name)\n",
    "    os.makedirs(output_sub_dir_path, exist_ok=True)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを処理\n",
    "    files = os.listdir(sub_dir_path)\n",
    "    for i, filename in enumerate(tqdm(sorted(files), desc=f\"Processing {sub_dir_name}\", unit=\"file\"), start=1):\n",
    "        file_path = os.path.join(sub_dir_path, filename)\n",
    "        \n",
    "        # ファイルを読み込む\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 音声合成\n",
    "        result = text2speech(content)\n",
    "        \n",
    "        # 音声ファイルの保存\n",
    "        output_file_path = os.path.join(output_sub_dir_path, f\"{i:03}.wav\")\n",
    "        sf.write(output_file_path, result[\"wav\"], text2speech.fs, \"PCM_16\")\n",
    "\n",
    "print(\"音声ファイルの作成が完了しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
